{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff331e1-6f39-4ae5-bf5f-b6f98e3f818d",
   "metadata": {},
   "source": [
    "## Class for creating the data set and function to form PyTorch DataLoaders with the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a73ae5-a784-4b9f-a966-d65c8516f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import optuna\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "# This routine takes a set of maps and remove their monopole (i.e. average value)\n",
    "def remove_monopole(maps, verbose=True):\n",
    "\n",
    "    if verbose:  print('removing monopoles')\n",
    "\n",
    "    # compute the mean of each map\n",
    "    maps_mean = np.mean(maps, axis=(1,2), dtype=np.float64)\n",
    "\n",
    "    # do a loop over all maps and remove mean value\n",
    "    for i in range(maps.shape[0]):\n",
    "          maps[i] = maps[i] - maps_mean[i]\n",
    "\n",
    "    return maps\n",
    "\n",
    "\n",
    "# This class creates the dataset. It will read the maps and store them in memory\n",
    "# the rotations and flipings are done when calling the data \n",
    "\n",
    "class make_dataset2(Dataset):\n",
    "    def __init__(self, mode, seed, fmaps, fparams, splits, fmaps_norm, \n",
    "                 monopole, monopole_norm, verbose):\n",
    "        super().__init__()\n",
    "\n",
    "        # getting the total number of simulations and maps\n",
    "        # there are 1000 simulations and each simulation has 15 maps\n",
    "        # we have selected some maps per simulations using 'splits'\n",
    "\n",
    "        # loading SIMULATION parameters:\n",
    "        params_sims = np.loadtxt(fparams)\n",
    "        total_sims, total_maps, num_params = params_sims.shape[0], params_sims.shape[0]*splits, params_sims.shape[1]\n",
    "\n",
    "        # initialising array for MAP parameters:\n",
    "        params_maps = np.zeros((total_maps, num_params), dtype=np.float32)\n",
    "\n",
    "        # loading the map parameters into the array:\n",
    "        for i in range(total_sims):\n",
    "            for j in range(splits):\n",
    "                params_maps[i*splits + j] = params_sims[i]\n",
    "\n",
    "        # normalizing the the cosmological & astrophysical parameters for each map (min-max)\n",
    "        # total of 6 parameters (2 cosmological and 4 astrophysical)\n",
    "\n",
    "        minimum     = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "        maximum     = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "        params_maps = (params_maps - minimum)/(maximum - minimum)\n",
    "\n",
    "        # get the size and offset depending on the type of dataset\n",
    "        if   mode=='train':  offset, size_sims = int(0.00*total_sims), int(0.90*total_sims)\n",
    "        elif mode=='valid':  offset, size_sims = int(0.90*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='test':   offset, size_sims = int(0.95*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='all':    offset, size_sims = int(0.00*total_sims), int(1.00*total_sims)\n",
    "        else:                raise Exception('Wrong name!')\n",
    "\n",
    "        # total size of maps is total size of simulations in the dataset mode (train/valid/test) multiplied by splits\n",
    "        size_maps = size_sims*splits\n",
    "\n",
    "\n",
    "        # randomly shuffle the simulations (not maps). Instead of 0 1 2 3...999 have a \n",
    "        # random permutation. E.g. 5 9 0 29...342\n",
    "        np.random.seed(seed)\n",
    "        sim_numbers = np.arange(total_sims) #shuffle sims not maps\n",
    "        np.random.shuffle(sim_numbers)\n",
    "        sim_numbers = sim_numbers[offset:offset+size_sims] #select indexes of mode\n",
    "    \n",
    "        # after shuffling the SIMULATIONS,getting the corresponding indexes of the MAPS associated to the simulations:\n",
    "        indexes = np.zeros(size_maps, dtype=np.int32)\n",
    "        count = 0\n",
    "        for i in sim_numbers:\n",
    "            for j in range(splits):\n",
    "                indexes[count] = i*splits + j\n",
    "                count += 1\n",
    "\n",
    "        # using the parameters of the maps with the selected indices\n",
    "        params_maps = params_maps[indexes]\n",
    "\n",
    "        # loading the map data\n",
    "\n",
    "        # length of the list is the number of channels\n",
    "        # e.g., if there are say T and Mtot for IllustrisTNG, then it is a multifield map with 2 channels\n",
    "        channels = len(fmaps)\n",
    "\n",
    "        # loading the first map in fmaps list\n",
    "        dumb = np.load(fmaps[0])    \n",
    "\n",
    "        # height and width of the first map in fmaps list\n",
    "        height, width = dumb.shape[1], dumb.shape[2]\n",
    "        del dumb\n",
    "\n",
    "        # initialising the data array which accounts for the number of channels as a separate axis\n",
    "        data = np.zeros((size_maps, channels, height, width), dtype=np.float32)\n",
    "\n",
    "        # actually reading the data\n",
    "    \n",
    "        print('Found %d channels\\nReading data...'%channels)\n",
    "\n",
    "        for channel, (fim, fnorm) in enumerate(zip(fmaps, fmaps_norm)):\n",
    "\n",
    "            # read maps in the channel in the current channel\n",
    "            data_c = np.load(fim)\n",
    "            if data_c.shape[0]!=total_maps:\n",
    "                raise Exception('sizes do not match')\n",
    "            if verbose:\n",
    "                print('%.3e < F(all|original) < %.3e'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "\n",
    "            # rescale maps (log scale)\n",
    "            # replacing only the 0 value pixels with 1\n",
    "            data_c = np.where(data_c !=0, data_c, 1)\n",
    "            # scaling logarithmically whilst preserving the sign\n",
    "            data_c = np.sign(data_c)*np.log10(np.abs(data_c))\n",
    "\n",
    "            if verbose:\n",
    "                print('%.3f < F(all|rescaled)  < %.3f'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "            # remove monopole of the images\n",
    "            if monopole is False:\n",
    "                data_c = remove_monopole(data_c, verbose)\n",
    "\n",
    "            # normalize maps (mean,std)\n",
    "            # fnorm contains information about normalising the data with respect to another dataset. E.g., training on TNG and testing on SIMBA\n",
    "            if fnorm is None:  \n",
    "                mean,std = np.mean(data_c), np.std(data_c)\n",
    "                #minimum, maximum = np.min(data_c),  np.max(data_c)\n",
    "            else:\n",
    "                # read data\n",
    "                data_norm = np.load(fnorm)\n",
    "\n",
    "                # rescale\n",
    "                data_norm = np.where(data_norm !=0, data_norm, 1)\n",
    "                data_norm = np.sign(data_norm)*np.log10(np.abs(data_norm))\n",
    "\n",
    "                # remove monopole\n",
    "                if monopole_norm is False:\n",
    "                    data_norm = remove_monopole(data_norm, verbose)\n",
    "\n",
    "                # compute mean and std\n",
    "                mean, std = np.mean(data_norm), np.std(data_norm)\n",
    "                minimum, maximum = np.min(data_norm),  np.max(data_norm)\n",
    "\n",
    "                #deleting data_norm from memory\n",
    "                del data_norm\n",
    "\n",
    "            data_c = (data_c - mean)/std\n",
    "            if verbose:\n",
    "                print('%.3f < F(all|normalized) < %.3f'%(np.min(data_c), np.max(data_c))) \n",
    "\n",
    "            # keep only the data of the chosen indices as the params\n",
    "            # loading the data for each channel into the data array per channel axis\n",
    "\n",
    "            data[:,channel,:,:] = data_c[indexes]\n",
    "\n",
    "            if verbose:\n",
    "                print('Channel %d contains %d maps'%(channel,size_maps))\n",
    "                print('%.3f < F < %.3f'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "        self.size = data.shape[0]\n",
    "        self.x    = torch.from_numpy(data)\n",
    "        self.y    = torch.from_numpy(params_maps)\n",
    "        del data, data_c, params_maps, params_sims\n",
    "\n",
    "        print('{} dataset created!\\n'.format(mode))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "        del self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # choosing a rotation angle (0 = 0째, 1 = 90째, 2 = 180째, 3 = 270째)\n",
    "        # and whether flipping is done or not\n",
    "        rot  = np.random.randint(0,4)\n",
    "        flip = np.random.randint(0,1)\n",
    "\n",
    "        # rotate and flip the maps\n",
    "        maps = torch.rot90(self.x[idx], k=rot, dims=[1,2])\n",
    "        if flip==1:  maps = torch.flip(maps, dims=[1])\n",
    "\n",
    "        return maps, self.y[idx]\n",
    "        \n",
    "        del maps, self.x[idx], self.y[idx], rot, flip\n",
    "\n",
    "# This class creates the dataset. Rotations and flippings are done and stored\n",
    "class make_dataset(Dataset):\n",
    "    def __init__(self, mode, seed, fmaps, fparams, splits, fmaps_norm, \n",
    "                 monopole, monopole_norm, just_monopole, verbose):\n",
    "        super().__init__()\n",
    "\n",
    "        # getting the total number of simulations and maps\n",
    "        # there are 1000 simulations and each simulation has 15 maps\n",
    "        # we have selected some maps per simulations using 'splits'\n",
    "\n",
    "        # loading SIMULATION parameters:\n",
    "        params_sims = np.loadtxt(fparams)\n",
    "        total_sims, total_maps, num_params = params_sims.shape[0], params_sims.shape[0]*splits, params_sims.shape[1]\n",
    "\n",
    "        # initialising array for MAP parameters:\n",
    "        params_maps = np.zeros((total_maps, num_params), dtype=np.float32)\n",
    "\n",
    "        # loading the map parameters into the array:\n",
    "        for i in range(total_sims):\n",
    "            for j in range(splits):\n",
    "                params_maps[i*splits + j] = params_sims[i]\n",
    "\n",
    "        # normalizing the the cosmological & astrophysical parameters for each map (min-max)\n",
    "        # total of 6 parameters (2 cosmological and 4 astrophysical)\n",
    "\n",
    "        minimum     = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "        maximum     = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "        params_maps = (params_maps - minimum)/(maximum - minimum)\n",
    "\n",
    "        # get the size and offset depending on the type of dataset\n",
    "        if   mode=='train':  offset, size_sims = int(0.00*total_sims), int(0.90*total_sims)\n",
    "        elif mode=='valid':  offset, size_sims = int(0.90*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='test':   offset, size_sims = int(0.95*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='all':    offset, size_sims = int(0.00*total_sims), int(1.00*total_sims)\n",
    "        else:                raise Exception('Wrong name!')\n",
    "\n",
    "        # total size of maps is total size of simulations in the dataset mode (train/valid/test) multiplied by splits\n",
    "        size_maps = size_sims*splits\n",
    "\n",
    "\n",
    "        # randomly shuffle the simulations (not maps). Instead of 0 1 2 3...999 have a \n",
    "        # random permutation. E.g. 5 9 0 29...342\n",
    "        np.random.seed(seed)\n",
    "        sim_numbers = np.arange(total_sims) #shuffle sims not maps\n",
    "        np.random.shuffle(sim_numbers)\n",
    "        sim_numbers = sim_numbers[offset:offset+size_sims] #select indexes of mode\n",
    "    \n",
    "        # after shuffling the SIMULATIONS,getting the corresponding indexes of the MAPS associated to the simulations:\n",
    "        indexes = np.zeros(size_maps, dtype=np.int32)\n",
    "        count = 0\n",
    "        for i in sim_numbers:\n",
    "            for j in range(splits):\n",
    "                indexes[count] = i*splits + j\n",
    "                count += 1\n",
    "\n",
    "        # using the parameters of the maps with the selected indices\n",
    "        params_maps = params_maps[indexes]\n",
    "\n",
    "        # loading the map data\n",
    "\n",
    "        # length of the list is the number of channels\n",
    "        # e.g., if there are say T and Mtot for IllustrisTNG, then it is a multifield map with 2 channels\n",
    "        channels = len(fmaps)\n",
    "\n",
    "        # loading the first map in fmaps list\n",
    "        dumb = np.load(fmaps[0])    \n",
    "\n",
    "        # height and width of the first map in fmaps list\n",
    "        height, width = dumb.shape[1], dumb.shape[2]\n",
    "        del dumb\n",
    "\n",
    "        # initialising the data array which accounts for the number of channels as a separate axis\n",
    "        data     = np.zeros((size_maps*8, channels, height, width), dtype=np.float32)\n",
    "        params   = np.zeros((size_maps*8, num_params),              dtype=np.float32)\n",
    "\n",
    "        # actually reading the data\n",
    "    \n",
    "        print('Found %d channels\\nReading data...'%channels)\n",
    "\n",
    "        for channel, (fim, fnorm) in enumerate(zip(fmaps, fmaps_norm)):\n",
    "\n",
    "            # read maps in the channel in the current channel\n",
    "            data_c = np.load(fim)\n",
    "            if data_c.shape[0]!=total_maps:\n",
    "                raise Exception('sizes do not match')\n",
    "            if verbose:\n",
    "                print('%.3e < F(all|original) < %.3e'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "\n",
    "            # rescale maps (log scale)\n",
    "            # replacing only the 0 value pixels with 1\n",
    "            data_c = np.where(data_c !=0, data_c, 1)\n",
    "            # scaling logarithmically whilst preserving the sign\n",
    "            data_c = np.sign(data_c)*np.log10(np.abs(data_c))\n",
    "\n",
    "            if verbose:\n",
    "                print('%.3f < F(all|rescaled)  < %.3f'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "            # remove monopole of the images\n",
    "            if monopole is False:\n",
    "                data_c = remove_monopole(data_c, verbose)\n",
    "\n",
    "            # normalize maps (mean,std)\n",
    "            # fnorm contains information about normalising the data with respect to another dataset. E.g., training on TNG and testing on SIMBA\n",
    "            if fnorm is None:  \n",
    "                mean,std = np.mean(data_c), np.std(data_c)\n",
    "                minimum, maximum = np.min(data_c),  np.max(data_c)\n",
    "            else:\n",
    "                # read data\n",
    "                data_norm = np.load(fnorm)\n",
    "\n",
    "                # rescale\n",
    "                data_norm = np.where(data_norm !=0, data_norm, 1)\n",
    "                data_norm = np.sign(data_norm)*np.log10(np.abs(data_norm))\n",
    "\n",
    "                # remove monopole\n",
    "                if monopole_norm is False:\n",
    "                    data_norm = remove_monopole(data_norm, verbose)\n",
    "\n",
    "                # compute mean and std\n",
    "                mean, std = np.mean(data_norm), np.std(data_norm)\n",
    "                minimum, maximum = np.min(data_norm),  np.max(data_norm)\n",
    "\n",
    "                #deleting data_norm from memory\n",
    "                del data_norm\n",
    "\n",
    "                # whether to make maps with the mean value in all pixels\n",
    "                if just_monopole:\n",
    "                    data_c = 10**(data_c)\n",
    "                    mean_each_map = np.mean(data_c, axis=(1,2))\n",
    "                    for i in range(data_c.shape[0]):\n",
    "                        data_c[i] = mean_each_map[i]\n",
    "                    data_c = np.log10(data_c)\n",
    "\n",
    "            data_c = (data_c - mean)/std\n",
    "            if verbose:\n",
    "                print('%.3f < F(all|normalized) < %.3f'%(np.min(data_c), np.max(data_c))) \n",
    "\n",
    "            # keep only the data of the chosen indices as the params\n",
    "            # loading the data for each channel into the data array per channel axis\n",
    "\n",
    "            data_c = data_c[indexes]\n",
    "\n",
    "            # do a loop over all rotations (each is 90 deg)\n",
    "            counted_maps = 0\n",
    "            for rot in [0,1,2,3]:\n",
    "                data_rot = np.rot90(data_c, k=rot, axes=(1,2))\n",
    "\n",
    "                data[counted_maps:counted_maps+size_maps,channel,:,:] = data_rot\n",
    "                params[counted_maps:counted_maps+size_maps]           = params_maps\n",
    "                counted_maps += size_maps\n",
    "\n",
    "                data[counted_maps:counted_maps+size_maps,channel,:,:] = \\\n",
    "                                                    np.flip(data_rot, axis=1)\n",
    "                params[counted_maps:counted_maps+size_maps]           = params_maps\n",
    "                counted_maps += size_maps\n",
    "\n",
    "                del data_rot\n",
    "\n",
    "            if verbose:\n",
    "                print('Channel %d contains %d maps'%(channel,counted_maps))\n",
    "                print('%.3f < F < %.3f'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "        del data_c, params_maps, params_sims\n",
    "\n",
    "        self.size = data.shape[0]\n",
    "        self.x    = torch.from_numpy(data)\n",
    "        self.y    = torch.from_numpy(params)\n",
    "\n",
    "        del data, params\n",
    "\n",
    "        print('{} dataset created!\\n'.format(mode))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.x[idx], self.y[idx]\n",
    "        del self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader(mode, seed, fmaps, fparams, batch_size, splits, \n",
    "                      fmaps_norm, monopole=True, monopole_norm=True,\n",
    "                      rot_flip_in_mem=True, shuffle=True, \n",
    "                      just_monopole=False, verbose=False):\n",
    "\n",
    "    # whether rotations and flippings are kept in memory\n",
    "    if rot_flip_in_mem:\n",
    "        data_set = make_dataset(mode, seed, fmaps, fparams, splits, \n",
    "                                fmaps_norm, monopole, monopole_norm, \n",
    "                                just_monopole, verbose)\n",
    "    else:\n",
    "        data_set = make_dataset2(mode, seed, fmaps, fparams, splits, \n",
    "                                fmaps_norm, monopole, monopole_norm, \n",
    "                                verbose)\n",
    "\n",
    "    data_loader = DataLoader(dataset=data_set, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6326f-0f08-48ea-9fc6-7c18b251e433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test8",
   "language": "python",
   "name": "test8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
